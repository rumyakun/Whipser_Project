# ğŸ¤ í™”ì êµ¬ë¶„ & ê°ì • ë¶„ì„ ì¢…í•© ê°€ì´ë“œ

## ğŸ“– ê°œìš”

í™”ì êµ¬ë¶„(Speaker Diarization)ê³¼ ê°ì • ë¶„ì„(Emotion Recognition)ì€ ê°ê° ë‹¤ë¥¸ ì „ìš© ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê³ ê¸‰ ìŒì„± ì²˜ë¦¬ ê¸°ìˆ ì…ë‹ˆë‹¤.

## ğŸ¯ í™”ì êµ¬ë¶„ (Speaker Diarization)

### ğŸ”§ ì‘ë™ ì›ë¦¬

1. **í™”ì ì„ë² ë”© ìƒì„±**
   - LSTM + ì–´í…ì…˜ ê¸°ë°˜ ì‹ ê²½ë§
   - MFCC íŠ¹ì„±ì—ì„œ í™”ìë³„ ê³ ìœ  ë²¡í„° ì¶”ì¶œ
   - 256ì°¨ì› ì„ë² ë”© ê³µê°„ì—ì„œ í™”ì êµ¬ë¶„

2. **í´ëŸ¬ìŠ¤í„°ë§**
   - K-means ë˜ëŠ” Agglomerative Clustering
   - í™”ì ìˆ˜ ìë™ ì¶”ì • (Silhouette Score ê¸°ë°˜)
   - ì‹¤ì‹œê°„ í™”ì ë³€ê²½ ê°ì§€

### ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°

```python
SpeakerEncoder:
â”œâ”€â”€ LSTM (3 layers, bidirectional)
â”œâ”€â”€ Multi-head Attention (8 heads)
â”œâ”€â”€ Embedding Layer (256 dimensions)
â””â”€â”€ Layer Normalization
```

### ğŸ“Š ì£¼ìš” ê¸°ëŠ¥

- **ì‹¤ì‹œê°„ í™”ì ë³€ê²½ ê°ì§€**
- **í™”ì ìˆ˜ ìë™ ì¶”ì •**
- **í™”ìë³„ ì‹ ë¢°ë„ ê³„ì‚°**
- **í™”ì í†µê³„ ë¶„ì„**

### ğŸ›ï¸ ì£¼ìš” ë§¤ê°œë³€ìˆ˜

```python
# í™”ì êµ¬ë¶„ ì„¤ì •
speaker_diarization = SpeakerDiarization(
    model_path='speaker_model.pth',
    sample_rate=16000,
    frame_duration=0.025,      # 25ms í”„ë ˆì„
    embedding_size=256,        # ì„ë² ë”© í¬ê¸°
    device='cpu'
)

# í™”ì ë³€ê²½ ì„ê³„ê°’
speaker_diarization.speaker_change_threshold = 0.7
```

## ğŸ˜Š ê°ì • ë¶„ì„ (Emotion Recognition)

### ğŸ”§ ì‘ë™ ì›ë¦¬

1. **ê°ì • íŠ¹ì„± ì¶”ì¶œ**
   - MFCC (13ì°¨ì›)
   - ìŠ¤í™íŠ¸ëŸ¼ ì¤‘ì‹¬ ì£¼íŒŒìˆ˜
   - ìŠ¤í™íŠ¸ëŸ¼ ë¡¤ì˜¤í”„
   - ì œë¡œ í¬ë¡œì‹± ë ˆì´íŠ¸
   - ìŠ¤í™íŠ¸ëŸ¼ ëŒ€ë¹„

2. **ê°ì • ë¶„ë¥˜**
   - LSTM + ì–´í…ì…˜ ê¸°ë°˜ ë¶„ë¥˜ê¸°
   - 7ê°€ì§€ ê°ì • í´ë˜ìŠ¤ ë¶„ë¥˜
   - ì‹¤ì‹œê°„ ê°ì • ë³€í™” ì¶”ì 

### ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°

```python
EmotionClassifier:
â”œâ”€â”€ Feature Extractor (128 â†’ 256)
â”œâ”€â”€ LSTM (3 layers, bidirectional)
â”œâ”€â”€ Multi-head Attention (8 heads)
â””â”€â”€ Classifier (256 â†’ 128 â†’ 64 â†’ 7)
```

### ğŸ“Š ê°ì • í´ë˜ìŠ¤

| ê°ì • | í•œêµ­ì–´ | ìƒ‰ìƒ | ì„¤ëª… |
|------|--------|------|------|
| angry | ë¶„ë…¸ | ğŸ”´ ë¹¨ê°• | í™”ë‚œ ëª©ì†Œë¦¬ |
| disgust | í˜ì˜¤ | ğŸŸ« ê°ˆìƒ‰ | ì‹«ì–´í•˜ëŠ” ëª©ì†Œë¦¬ |
| fear | ë‘ë ¤ì›€ | ğŸŸ£ ë³´ë¼ | ë¬´ì„œì›Œí•˜ëŠ” ëª©ì†Œë¦¬ |
| happy | ê¸°ì¨ | ğŸŸ¡ ë…¸ë‘ | ì¦ê±°ìš´ ëª©ì†Œë¦¬ |
| sad | ìŠ¬í”” | ğŸ”µ íŒŒë‘ | ìŠ¬í”ˆ ëª©ì†Œë¦¬ |
| surprise | ë†€ëŒ | ğŸŸ  ì£¼í™© | ë†€ë€ ëª©ì†Œë¦¬ |
| neutral | ì¤‘ë¦½ | âšª íšŒìƒ‰ | í‰ì˜¨í•œ ëª©ì†Œë¦¬ |

### ğŸ›ï¸ ì£¼ìš” ë§¤ê°œë³€ìˆ˜

```python
# ê°ì • ë¶„ì„ ì„¤ì •
emotion_recognition = EmotionRecognition(
    model_path='emotion_model.pth',
    sample_rate=16000,
    frame_duration=0.025,      # 25ms í”„ë ˆì„
    device='cpu'
)

# ê°ì • íˆìŠ¤í† ë¦¬ í¬ê¸°
emotion_recognition.emotion_history = deque(maxlen=50)
```

## ğŸ”„ í†µí•© ì²˜ë¦¬ (Integrated Audio Processor)

### ğŸ¯ í†µí•© ê¸°ëŠ¥

```python
processor = IntegratedAudioProcessor(
    speaker_model_path='speaker_model.pth',
    emotion_model_path='emotion_model.pth',
    enable_speaker_diarization=True,
    enable_emotion_recognition=True,
    enable_vad=True
)
```

### ğŸ“Š ì‹¤ì‹œê°„ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```
ì˜¤ë””ì˜¤ ì…ë ¥ â†’ VAD â†’ í™”ì êµ¬ë¶„ â†’ ê°ì • ë¶„ì„ â†’ í†µí•© ê²°ê³¼
    â†“           â†“         â†“         â†“         â†“
ìŒì„± ê°ì§€   í™”ì ë³€ê²½   ê°ì • ë³€í™”   í†µê³„ ë¶„ì„   ì½œë°± í˜¸ì¶œ
```

### ğŸ›ï¸ ì½œë°± ì‹œìŠ¤í…œ

```python
def on_speaker_change(speaker_info):
    print(f"í™”ì ë³€ê²½: {speaker_info}")

def on_emotion_change(emotion_info):
    print(f"ê°ì • ë³€ê²½: {emotion_info['current_emotion']}")

def on_vad_change(vad_info):
    print(f"ìŒì„± ê°ì§€: {vad_info}")

processor.set_callbacks(
    on_speaker_change=on_speaker_change,
    on_emotion_change=on_emotion_change,
    on_vad_change=on_vad_change
)
```

## ğŸš€ ì‚¬ìš©ë²•

### 1ï¸âƒ£ ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from integrated_audio_processor import IntegratedAudioProcessor

# í”„ë¡œì„¸ì„œ ìƒì„±
processor = IntegratedAudioProcessor()

# ì½œë°± ì„¤ì •
def on_result(result):
    print(f"í™”ì: {result.get('speaker_info')}")
    print(f"ê°ì •: {result.get('emotion_info', {}).get('current_emotion')}")

processor.set_callbacks(on_integrated_result=on_result)

# ì²˜ë¦¬ ì‹œì‘
processor.start_processing()

# 30ì´ˆ í›„ ë¶„ì„
import time
time.sleep(30)
analysis = processor.get_integrated_analysis(duration=30.0)
print(analysis)

# ì²˜ë¦¬ ì¤‘ì§€
processor.stop_processing()
```

### 2ï¸âƒ£ ê³ ê¸‰ ì‚¬ìš©ë²•

```python
# ì»¤ìŠ¤í…€ ì„¤ì •
processor = IntegratedAudioProcessor(
    speaker_model_path='custom_speaker_model.pth',
    emotion_model_path='custom_emotion_model.pth',
    enable_speaker_diarization=True,
    enable_emotion_recognition=True,
    enable_vad=True
)

# ì‹¤ì‹œê°„ í†µê³„ ëª¨ë‹ˆí„°ë§
while True:
    status = processor.get_processing_status()
    print(f"í˜„ì¬ í™”ì: {status['current_speaker']}")
    print(f"í˜„ì¬ ê°ì •: {status['current_emotion']}")
    
    # ê°ì • ì¶”ì„¸ ë¶„ì„
    trend = processor.get_emotion_trend(window_size=20)
    print(f"ê°ì • ì¶”ì„¸: {trend['trend']}")
    
    time.sleep(1)
```

### 3ï¸âƒ£ ê²°ê³¼ ì €ì¥ ë° ë¡œë“œ

```python
# ê²°ê³¼ ì €ì¥
processor.save_results('analysis_results.json')

# ê²°ê³¼ ë¡œë“œ
processor.load_results('analysis_results.json')

# í†µê³„ ì •ë³´
speaker_stats = processor.get_speaker_statistics()
emotion_stats = processor.get_emotion_statistics()
print(f"í™”ì í†µê³„: {speaker_stats}")
print(f"ê°ì • í†µê³„: {emotion_stats}")
```

## ğŸ¨ GUI í†µí•©

### PySide6 ìœ„ì ¯ ì˜ˆì‹œ

```python
from PySide6.QtWidgets import QWidget, QVBoxLayout, QLabel
from PySide6.QtCore import QTimer

class AudioAnalysisWidget(QWidget):
    def __init__(self):
        super().__init__()
        self.processor = IntegratedAudioProcessor()
        self.setup_ui()
        self.setup_processor()
    
    def setup_ui(self):
        layout = QVBoxLayout()
        
        self.speaker_label = QLabel("í™”ì: ì—†ìŒ")
        self.emotion_label = QLabel("ê°ì •: ì—†ìŒ")
        self.confidence_label = QLabel("ì‹ ë¢°ë„: 0%")
        
        layout.addWidget(self.speaker_label)
        layout.addWidget(self.emotion_label)
        layout.addWidget(self.confidence_label)
        
        self.setLayout(layout)
    
    def setup_processor(self):
        self.processor.set_callbacks(
            on_speaker_change=self.on_speaker_change,
            on_emotion_change=self.on_emotion_change
        )
        
        # íƒ€ì´ë¨¸ë¡œ UI ì—…ë°ì´íŠ¸
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_ui)
        self.timer.start(100)  # 100msë§ˆë‹¤ ì—…ë°ì´íŠ¸
    
    def on_speaker_change(self, speaker_info):
        self.speaker_label.setText(f"í™”ì: {speaker_info.get('speaker_id', 'Unknown')}")
    
    def on_emotion_change(self, emotion_info):
        emotion = emotion_info.get('current_emotion', 'neutral')
        confidence = emotion_info.get('confidence', 0.0)
        
        self.emotion_label.setText(f"ê°ì •: {emotion}")
        self.confidence_label.setText(f"ì‹ ë¢°ë„: {confidence:.1%}")
    
    def update_ui(self):
        # ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸
        pass
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### 1ï¸âƒ£ ëª¨ë¸ ìµœì í™”

```python
# GPU ì‚¬ìš©
processor = IntegratedAudioProcessor(device='cuda')

# ë°°ì¹˜ ì²˜ë¦¬
processor.batch_size = 32

# ëª¨ë¸ ì–‘ìí™” (ì„ íƒì‚¬í•­)
import torch.quantization
quantized_model = torch.quantization.quantize_dynamic(
    processor.speaker_diarization.encoder,
    {torch.nn.Linear},
    dtype=torch.qint8
)
```

### 2ï¸âƒ£ ë©”ëª¨ë¦¬ ìµœì í™”

```python
# ë²„í¼ í¬ê¸° ì¡°ì •
processor.audio_buffer = deque(maxlen=16000 * 5)  # 5ì´ˆ ë²„í¼

# ê²°ê³¼ íˆìŠ¤í† ë¦¬ ì œí•œ
processor.results['integrated_results'] = processor.results['integrated_results'][-1000:]
```

### 3ï¸âƒ£ ì‹¤ì‹œê°„ ì„±ëŠ¥

```python
# ì²˜ë¦¬ ì£¼ê¸° ì¡°ì •
processor.processing_interval = 0.02  # 20ms

# ë©€í‹°ìŠ¤ë ˆë”©
processor.num_threads = 4
```

## ğŸ”§ ëª¨ë¸ í›ˆë ¨

### 1ï¸âƒ£ í™”ì êµ¬ë¶„ ëª¨ë¸ í›ˆë ¨

```python
from speaker_diarization import train_speaker_model

# í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
speaker_labels = {
    'speaker1_audio1': 'speaker1',
    'speaker1_audio2': 'speaker1',
    'speaker2_audio1': 'speaker2',
    'speaker2_audio2': 'speaker2'
}

# ëª¨ë¸ í›ˆë ¨
train_speaker_model(
    audio_dir='speaker_training_data',
    speaker_labels=speaker_labels,
    model_save_path='speaker_model.pth'
)
```

### 2ï¸âƒ£ ê°ì • ë¶„ì„ ëª¨ë¸ í›ˆë ¨

```python
from emotion_recognition import train_emotion_model

# í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
emotion_labels = {
    'happy_audio1': 3,    # happy
    'sad_audio1': 4,      # sad
    'angry_audio1': 0,    # angry
    'neutral_audio1': 6   # neutral
}

# ëª¨ë¸ í›ˆë ¨
train_emotion_model(
    audio_dir='emotion_training_data',
    emotion_labels=emotion_labels,
    model_save_path='emotion_model.pth'
)
```

## ğŸ“Š ë¶„ì„ ê²°ê³¼ ì˜ˆì‹œ

### í†µí•© ë¶„ì„ ê²°ê³¼

```json
{
  "duration": 30.0,
  "total_processed_chunks": 1200,
  "speaker_analysis": {
    "total_speaker_changes": 5,
    "current_speaker": "Speaker_1",
    "speaker_statistics": {
      "total_segments": 1200,
      "unique_speakers": 3,
      "speaker_distribution": {
        "Speaker_0": 400,
        "Speaker_1": 500,
        "Speaker_2": 300
      }
    }
  },
  "emotion_analysis": {
    "current_emotion": "happy",
    "emotion_trend": {
      "trend": "moderate",
      "dominant_emotion": "happy",
      "stability": 0.75
    },
    "emotion_intensity": 0.68,
    "emotion_statistics": {
      "total_analyses": 1200,
      "emotion_distribution": {
        "happy": 600,
        "neutral": 400,
        "sad": 200
      },
      "average_confidence": 0.82
    }
  },
  "vad_analysis": {
    "speech_ratio": 0.85,
    "total_speech_segments": 1020,
    "total_segments": 1200
  }
}
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1ï¸âƒ£ ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­

- **CPU ì‚¬ìš©ëŸ‰**: ì‹¤ì‹œê°„ ì²˜ë¦¬ ì‹œ ë†’ì€ CPU ì‚¬ìš©ë¥ 
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ì˜¤ë””ì˜¤ ë²„í¼ì™€ ê²°ê³¼ ì €ì¥ìœ¼ë¡œ ì¸í•œ ë©”ëª¨ë¦¬ ì¦ê°€
- **ì§€ì—° ì‹œê°„**: ëª¨ë¸ ì¶”ë¡ ìœ¼ë¡œ ì¸í•œ ì•½ê°„ì˜ ì§€ì—°

### 2ï¸âƒ£ ì •í™•ë„ ì œí•œ

- **í™”ì êµ¬ë¶„**: ë¹„ìŠ·í•œ ëª©ì†Œë¦¬ êµ¬ë¶„ ì–´ë ¤ì›€
- **ê°ì • ë¶„ì„**: ë¬¸í™”ì  ì°¨ì´ì™€ ê°œì¸ì°¨ ê³ ë ¤ í•„ìš”
- **ë…¸ì´ì¦ˆ**: ë°°ê²½ ì†ŒìŒì— ë¯¼ê°í•  ìˆ˜ ìˆìŒ

### 3ï¸âƒ£ ë°ì´í„° ìš”êµ¬ì‚¬í•­

- **í›ˆë ¨ ë°ì´í„°**: ì¶©ë¶„í•œ í™”ìë³„/ê°ì •ë³„ ì˜¤ë””ì˜¤ ìƒ˜í”Œ í•„ìš”
- **ë°ì´í„° í’ˆì§ˆ**: ê³ í’ˆì§ˆ ì˜¤ë””ì˜¤ ë…¹ìŒ ê¶Œì¥
- **ë‹¤ì–‘ì„±**: ë‹¤ì–‘í•œ í™˜ê²½ê³¼ ì¡°ê±´ì˜ ë°ì´í„° í•„ìš”

## ğŸš€ í–¥í›„ ë°œì „ ë°©í–¥

### 1ï¸âƒ£ ëª¨ë¸ ê°œì„ 

- **Transformer ê¸°ë°˜ ëª¨ë¸**: ë” ì •í™•í•œ í™”ì/ê°ì • ì¸ì‹
- **ë©€í‹°ëª¨ë‹¬ ìœµí•©**: ìŒì„± + í‘œì • + ì œìŠ¤ì²˜ í†µí•© ë¶„ì„
- **ì ì‘í˜• í•™ìŠµ**: ì‹¤ì‹œê°„ ëª¨ë¸ ì—…ë°ì´íŠ¸

### 2ï¸âƒ£ ê¸°ëŠ¥ í™•ì¥

- **í™”ì ì¸ì¦**: íŠ¹ì • í™”ì ì‹ë³„ ë° ì¸ì¦
- **ê°ì • ê°•ë„ ì¸¡ì •**: ê°ì •ì˜ ì„¸ë°€í•œ ê°•ë„ ë¶„ì„
- **ëŒ€í™” ë¶„ì„**: í™”ì ê°„ ìƒí˜¸ì‘ìš© íŒ¨í„´ ë¶„ì„

### 3ï¸âƒ£ ì‹¤ìš©ì„± í–¥ìƒ

- **ì—ì§€ ë””ë°”ì´ìŠ¤ ìµœì í™”**: ëª¨ë°”ì¼/ì„ë² ë””ë“œ ì§€ì›
- **í´ë¼ìš°ë“œ ì—°ë™**: ì‹¤ì‹œê°„ í´ë¼ìš°ë“œ ë¶„ì„
- **API ì„œë¹„ìŠ¤**: RESTful API ì œê³µ

## ğŸ“š ì°¸ê³  ìë£Œ

- [Speaker Diarization Paper](https://arxiv.org/abs/2003.02405)
- [Emotion Recognition Survey](https://arxiv.org/abs/1803.08974)
- [PyTorch Audio Tutorials](https://pytorch.org/audio/stable/tutorials/index.html)
- [Librosa Documentation](https://librosa.org/doc/latest/index.html)

---

ì´ ê°€ì´ë“œë¥¼ í†µí•´ í™”ì êµ¬ë¶„ê³¼ ê°ì • ë¶„ì„ ê¸°ëŠ¥ì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ê³ ê¸‰ ìŒì„± ì¸ì‹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ‰ 