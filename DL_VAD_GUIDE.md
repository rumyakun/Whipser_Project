# ë”¥ëŸ¬ë‹ ê¸°ë°˜ VAD (Voice Activity Detection) ê°€ì´ë“œ

## ğŸ§  ë”¥ëŸ¬ë‹ VADë€?

ë”¥ëŸ¬ë‹ ê¸°ë°˜ VADëŠ” ê¸°ì¡´ì˜ ê·œì¹™ ê¸°ë°˜ VADë³´ë‹¤ í›¨ì”¬ ì •í™•í•œ ìŒì„± í™œë™ ê°ì§€ë¥¼ ì œê³µí•©ë‹ˆë‹¤. LSTMê³¼ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ì˜¤ë””ì˜¤ í™˜ê²½ì—ì„œë„ ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.

## ğŸ—ï¸ ë”¥ëŸ¬ë‹ VAD êµ¬ì¡°

### 1. **ì‹ ê²½ë§ ì•„í‚¤í…ì²˜**

```
ì…ë ¥ (MFCC íŠ¹ì„±) â†’ LSTM â†’ ì–´í…ì…˜ â†’ ë¶„ë¥˜ê¸° â†’ ì¶œë ¥ (ìŒì„± í™•ë¥ )
```

#### ì£¼ìš” êµ¬ì„± ìš”ì†Œ:
- **LSTM ë ˆì´ì–´**: ì‹œí€€ìŠ¤ ë°ì´í„° ì²˜ë¦¬
- **ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜**: ì¤‘ìš”í•œ í”„ë ˆì„ì— ì§‘ì¤‘
- **ë¶„ë¥˜ê¸°**: ìŒì„±/ë¬´ìŒ ì´ì§„ ë¶„ë¥˜

### 2. **íŠ¹ì„± ì¶”ì¶œ**

```python
# MFCC (Mel-frequency cepstral coefficients)
mfcc = librosa.feature.mfcc(y=audio, sr=16000, n_mfcc=13)

# ë¸íƒ€ íŠ¹ì„± (1ì°¨ ë¯¸ë¶„)
mfcc_delta = librosa.feature.delta(mfcc)

# ë¸íƒ€-ë¸íƒ€ íŠ¹ì„± (2ì°¨ ë¯¸ë¶„)
mfcc_delta2 = librosa.feature.delta(mfcc, order=2)

# ìµœì¢… íŠ¹ì„±: 39ì°¨ì› (13 + 13 + 13)
features = np.concatenate([mfcc, mfcc_delta, mfcc_delta2], axis=0)
```

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

| VAD íƒ€ì… | ì •í™•ë„ | ì²˜ë¦¬ ì†ë„ | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | í™˜ê²½ ì ì‘ì„± |
|----------|--------|-----------|---------------|-------------|
| **Simple VAD** | 70-80% | ë¹ ë¦„ | ë‚®ìŒ | ë‚®ìŒ |
| **Advanced VAD** | 80-85% | ë³´í†µ | ë³´í†µ | ë³´í†µ |
| **Deep Learning VAD** | 90-95% | ëŠë¦¼ | ë†’ìŒ | ë†’ìŒ |
| **Hybrid VAD** | 92-97% | ë³´í†µ | ë†’ìŒ | ë§¤ìš° ë†’ìŒ |

## ğŸš€ ì„¤ì¹˜ ë° ì„¤ì •

### 1. **í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜**

```bash
# PyTorch ì„¤ì¹˜
pip install torch torchvision torchaudio

# ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install librosa soundfile

# ê¸°íƒ€ ì˜ì¡´ì„±
pip install numpy scipy matplotlib
```

### 2. **GPU ì‚¬ìš© (ì„ íƒì‚¬í•­)**

```bash
# CUDA ì§€ì› PyTorch ì„¤ì¹˜ (GPU ì‚¬ìš© ì‹œ)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

## ğŸ¯ ì‚¬ìš© ë°©ë²•

### 1. **ê¸°ë³¸ ì‚¬ìš©ë²•**

```python
from deep_learning_vad import DeepLearningVAD

# VAD ì´ˆê¸°í™”
vad = DeepLearningVAD(
    model_path="vad_model.pth",  # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸
    sample_rate=16000,
    frame_duration=0.025
)

# ìŒì„± êµ¬ê°„ ê°ì§€ ì½œë°±
def on_speech_detected(audio_segment):
    print(f"ìŒì„± ê°ì§€: {len(audio_segment)} ìƒ˜í”Œ")

# VAD ì‹œì‘
vad.start_processing()

# ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²˜ë¦¬
while recording:
    audio_chunk = get_audio_chunk()
    segments = vad.process_audio_chunk(audio_chunk)
    
    for is_speech, audio_segment in segments:
        if is_speech:
            on_speech_detected(audio_segment)
```

### 2. **í–¥ìƒëœ VAD ì‚¬ìš©ë²•**

```python
from enhanced_vad_processor import EnhancedVADProcessor

# í•˜ì´ë¸Œë¦¬ë“œ VAD ì´ˆê¸°í™”
vad = EnhancedVADProcessor(
    sample_rate=16000,
    vad_type="hybrid",  # ì—¬ëŸ¬ VAD ê²°ê³¼ ê²°í•©
    callback=on_speech_detected
)

# VAD íƒ€ì… ë™ì  ë³€ê²½
vad.set_vad_type("deep")  # ë”¥ëŸ¬ë‹ VADë¡œ ë³€ê²½
vad.set_vad_type("hybrid")  # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œë¡œ ë³€ê²½

# í†µê³„ ì •ë³´ í™•ì¸
stats = vad.get_stats()
print(f"ê°ì§€ ì •í™•ë„: {stats['detection_accuracy']:.2%}")
```

### 3. **ì ì‘í˜• VAD ì‚¬ìš©ë²•**

```python
from enhanced_vad_processor import AdaptiveVADProcessor

# ì ì‘í˜• VAD ì´ˆê¸°í™” (í™˜ê²½ì— ë”°ë¼ ìë™ ì¡°ì •)
vad = AdaptiveVADProcessor(
    sample_rate=16000,
    callback=on_speech_detected
)

# í˜„ì¬ VAD ì •ë³´ í™•ì¸
info = vad.get_current_vad_info()
print(f"í˜„ì¬ VAD: {info['current_vad']}")
print(f"ë…¸ì´ì¦ˆ ë ˆë²¨: {info['noise_level']:.3f}")
print(f"ìŒì„± ê°ì§€ìœ¨: {info['speech_rate']:.3f}")
```

## ğŸ“ ëª¨ë¸ í›ˆë ¨

### 1. **ë°ì´í„° ì¤€ë¹„**

#### ì˜¤ë””ì˜¤ íŒŒì¼ êµ¬ì¡°:
```
data/
â”œâ”€â”€ audio/
â”‚   â”œâ”€â”€ sample1.wav
â”‚   â”œâ”€â”€ sample2.wav
â”‚   â””â”€â”€ ...
â””â”€â”€ labels/
    â”œâ”€â”€ sample1.txt
    â”œâ”€â”€ sample2.txt
    â””â”€â”€ ...
```

#### ë¼ë²¨ íŒŒì¼ í˜•ì‹:
```
0.0 1.5 speech
1.5 2.0 silence
2.0 4.2 speech
4.2 5.0 silence
```

### 2. **ëª¨ë¸ í›ˆë ¨**

```python
from deep_learning_vad import train_vad_model

# ëª¨ë¸ í›ˆë ¨
train_vad_model(
    audio_dir="data/audio",
    label_dir="data/labels",
    model_save_path="vad_model.pth"
)
```

### 3. **í›ˆë ¨ íŒŒë¼ë¯¸í„° ì¡°ì •**

```python
from deep_learning_vad import VADNet, VADTrainer
from torch.utils.data import DataLoader

# ëª¨ë¸ ìƒì„±
model = VADNet(
    input_size=39,      # MFCC íŠ¹ì„± ì°¨ì›
    hidden_size=64,     # LSTM íˆë“  í¬ê¸°
    num_layers=2        # LSTM ë ˆì´ì–´ ìˆ˜
)

# í›ˆë ¨ê¸° ìƒì„±
trainer = VADTrainer(model)

# ë°ì´í„° ë¡œë” ìƒì„±
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# í›ˆë ¨ ì‹¤í–‰
trainer.train(
    train_loader=train_loader,
    val_loader=val_loader,
    epochs=50,
    save_path="vad_model.pth"
)
```

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

### 1. **ì‹ ë¢°ë„ ì„ê³„ê°’ ì¡°ì •**

```python
# ë†’ì€ ì •í™•ë„ (ë‚®ì€ ì˜¤íƒ)
vad.set_confidence_threshold(0.8)

# ë†’ì€ ë¯¼ê°ë„ (ë‚®ì€ ë¯¸íƒ)
vad.set_confidence_threshold(0.3)

# ê¸°ë³¸ê°’
vad.set_confidence_threshold(0.7)
```

### 2. **ì‹¤ì‹œê°„ ì„±ëŠ¥ ìµœì í™”**

```python
# ë°°ì¹˜ í¬ê¸° ì¡°ì •
batch_size = 1  # ì‹¤ì‹œê°„ ì²˜ë¦¬ìš©

# ëª¨ë¸ ì–‘ìí™” (ì„ íƒì‚¬í•­)
import torch.quantization

quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
```

### 3. **ë©”ëª¨ë¦¬ ìµœì í™”**

```python
# ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… (í›ˆë ¨ ì‹œ)
model.use_checkpoint = True

# í˜¼í•© ì •ë°€ë„ í›ˆë ¨
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    output = model(input)
    loss = criterion(output, target)
```

## ğŸ“ˆ ì„±ëŠ¥ í‰ê°€

### 1. **ì •í™•ë„ ë©”íŠ¸ë¦­**

```python
def evaluate_vad_accuracy(predictions, ground_truth):
    """VAD ì •í™•ë„ í‰ê°€"""
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support
    
    accuracy = accuracy_score(ground_truth, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        ground_truth, predictions, average='binary'
    )
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1
    }
```

### 2. **ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¸¡ì •**

```python
import time

def measure_inference_time(vad, audio_chunk):
    """ì¶”ë¡  ì‹œê°„ ì¸¡ì •"""
    start_time = time.time()
    segments = vad.process_audio_chunk(audio_chunk)
    end_time = time.time()
    
    inference_time = (end_time - start_time) * 1000  # ms
    return inference_time, segments
```

## ğŸ¯ ìµœì í™” íŒ

### 1. **ë°ì´í„° í’ˆì§ˆ**
- ê³ í’ˆì§ˆ ë§ˆì´í¬ ì‚¬ìš©
- ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ë…¹ìŒ
- ì •í™•í•œ ë¼ë²¨ë§

### 2. **ëª¨ë¸ ì•„í‚¤í…ì²˜**
- ì ì ˆí•œ ëª¨ë¸ í¬ê¸° ì„ íƒ
- ë“œë¡­ì•„ì›ƒìœ¼ë¡œ ê³¼ì í•© ë°©ì§€
- ë°°ì¹˜ ì •ê·œí™” ì‚¬ìš©

### 3. **í›ˆë ¨ ì „ëµ**
- í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
- ì¡°ê¸° ì¢…ë£Œ (Early Stopping)
- ë°ì´í„° ì¦ê°•

### 4. **ì‹¤ì‹œê°„ ìµœì í™”**
- ëª¨ë¸ ì–‘ìí™”
- ë°°ì¹˜ ì²˜ë¦¬
- GPU ê°€ì†

## ğŸš¨ ì£¼ì˜ì‚¬í•­

1. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ë”¥ëŸ¬ë‹ VADëŠ” ìƒë‹¹í•œ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤
2. **ì²˜ë¦¬ ì§€ì—°**: ì‹¤ì‹œê°„ ì²˜ë¦¬ ì‹œ ì•½ê°„ì˜ ì§€ì—°ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
3. **GPU ì˜ì¡´ì„±**: GPU ì—†ì´ëŠ” ì²˜ë¦¬ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤
4. **ëª¨ë¸ í¬ê¸°**: ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ íŒŒì¼ì´ í´ ìˆ˜ ìˆìŠµë‹ˆë‹¤

## ğŸ”® í–¥í›„ ë°œì „ ë°©í–¥

1. **ê²½ëŸ‰í™” ëª¨ë¸**: ëª¨ë°”ì¼ í™˜ê²½ì„ ìœ„í•œ ê²½ëŸ‰ VAD
2. **ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ**: ìŒì„± ì¸ì‹ê³¼ VAD ë™ì‹œ í•™ìŠµ
3. **ìê¸° ì§€ë„ í•™ìŠµ**: ë¼ë²¨ ì—†ëŠ” ë°ì´í„°ë¡œ í•™ìŠµ
4. **ë„ë©”ì¸ ì ì‘**: íŠ¹ì • í™˜ê²½ì— ìµœì í™”ëœ VAD

ë”¥ëŸ¬ë‹ VADëŠ” ê¸°ì¡´ VADë³´ë‹¤ í›¨ì”¬ ì •í™•í•˜ì§€ë§Œ, ì ì ˆí•œ í•˜ë“œì›¨ì–´ì™€ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. í”„ë¡œì íŠ¸ì˜ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ì ì ˆí•œ VAD ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”! 